{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Dynamic Programming\n",
    "\n",
    "### Contents\n",
    "\n",
    " * Policy Evaluation (Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation\n",
    "\n",
    "Policy evaluation is is process of comupting the state-value function $v_{\\pi}$ for an arbitrary policy $\\pi$\n",
    "\n",
    "## Iteractive Policy evaluation\n",
    "\n",
    "In the following example we use 4x4 grid, where top-left and bottom-right corner have value 0 and are terminal states. \n",
    "\n",
    "The agent can move in NSEW directions and each move gives -1 reward.\n",
    "Lets calculate optimal value function for given policy in interactive manner.\n",
    "\n",
    "The algorithm can use 2 array to compute next iteration, but it can also work with single array in place updates. The end result is the same.\n",
    "\n",
    "Our agent uses equiprobable random policy (every action is equally probable).\n",
    "\n",
    "The update rule:\n",
    "$$ V(s) = \\sum_a{\\pi(a|s) \\sum_{s',r}{p(s',r|s,a)[r+\\gamma V(s')]}}$$\n",
    "\n",
    "Here $\\gamma = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=0\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "k=1\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "k=2\n",
      "[[ 0.   -1.75 -2.   -2.  ]\n",
      " [-1.75 -2.   -2.   -2.  ]\n",
      " [-2.   -2.   -2.   -1.75]\n",
      " [-2.   -2.   -1.75  0.  ]]\n",
      "k=3\n",
      "[[ 0.     -2.4375 -2.9375 -3.    ]\n",
      " [-2.4375 -2.875  -3.     -2.9375]\n",
      " [-2.9375 -3.     -2.875  -2.4375]\n",
      " [-3.     -2.9375 -2.4375  0.    ]]\n",
      "k=10\n",
      "[[ 0.         -6.13796997 -8.35235596 -8.96731567]\n",
      " [-6.13796997 -7.73739624 -8.42782593 -8.35235596]\n",
      " [-8.35235596 -8.42782593 -7.73739624 -6.13796997]\n",
      " [-8.96731567 -8.35235596 -6.13796997  0.        ]]\n",
      "k=100\n",
      "[[  0.         -13.94260509 -19.91495107 -21.90482522]\n",
      " [-13.94260509 -17.92507693 -19.91551999 -19.91495107]\n",
      " [-19.91495107 -19.91551999 -17.92507693 -13.94260509]\n",
      " [-21.90482522 -19.91495107 -13.94260509   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def update(grid):\n",
    "    out = np.zeros((4,4))\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            if i == j and (i == 0 or i == 3):\n",
    "                out[i,j] = 0\n",
    "            else:\n",
    "                l = grid[i,j] if i == 0 else grid[i-1,j]\n",
    "                r = grid[i,j] if i == 3 else grid[i+1,j]\n",
    "                t = grid[i,j] if j == 0 else grid[i,j-1]\n",
    "                b = grid[i,j] if j == 3 else grid[i,j+1]\n",
    "                out[i,j] = (l+r+t+b)/4 - 1\n",
    "    return out\n",
    "\n",
    "grid = np.zeros((4,4))\n",
    "print('k=0')\n",
    "print(grid)\n",
    "\n",
    "for k in range(100):\n",
    "    grid = update(grid)\n",
    "    if k in [0,1,2,9,99]:\n",
    "        print('k=%d' % (k+1))\n",
    "        print(grid)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal policy:\n",
    "<img src=\"../assets/policy1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy improvement\n",
    "\n",
    "The process of using greedy policy over original policy is called _policy improvement_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteraction\n",
    "\n",
    "When we switch to greedy policy in given state, we improve our policy.\n",
    "We can then take this new policy and calculate new value function.\n",
    "Then we switch to new greedy policy based on this new value functionn.\n",
    "\n",
    "This way we will find the _optimal policy_.\n",
    "\n",
    "## Example\n",
    "This is Exercise 4.5 from the Book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Car rental example goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteraction\n",
    "\n",
    "Description with Exercise 4.9"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
