{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Markov Decision Processes \n",
    "\n",
    "MDP formally describes environment for reinforcement learning. Where the environment is fully observable.\n",
    "\n",
    "MDP are important since almost all RL problems can be described as MDP (event partially observed).\n",
    "\n",
    "\n",
    "## Definition\n",
    "\n",
    "MDP is a process which contains Markov property. It means that the current state only depends on the previous state, \n",
    "not the whole history:\n",
    "\n",
    "$$ \\Pr( S_{t+1} \\mid S_{t} ) =  \\Pr( S_{t+1} \\mid S_1, ..., S_{t} ) $$\n",
    "\n",
    "\n",
    "### Markov Process (Markov chain)\n",
    "\n",
    "MP is a tuple $<S,P>$ where\n",
    " * S - List of all states\n",
    " * P - Transition probability table between states"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
