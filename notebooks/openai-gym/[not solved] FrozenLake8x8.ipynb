{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FrozenLake 8x8 - QTable\n",
    "\n",
    "**(This doesn't work yet)**\n",
    "\n",
    "This environment has 256 states. Because of this E-Greedy Agent takes a lot of episoded to learn the correct policy.\n",
    "To speed it up in this notebook we use UCB Agent. Based on Upper-Confidence-Bound Action Selection.\n",
    "For more information check [Sutton book](http://incompleteideas.net/sutton/book/bookdraft2016sep.pdf).\n",
    "\n",
    "For this tutorial we will use [Frozen Lake 8x8](https://gym.openai.com/envs/FrozenLake8x8-v0).\n",
    "\n",
    "### Solved\n",
    "FrozenLake is solved if moving average over window size 100 is >= 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "logging.getLogger('gym').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(xs, n=100):\n",
    "    ret = np.cumsum(xs, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "def find_index(xs, v):\n",
    "    \"\"\"Find index of the first value equal or greater then v\"\"\"\n",
    "    for i in range(len(xs)):\n",
    "        if xs[i] >= v:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def run_env(agent, env, num_episodes):\n",
    "    rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            a = agent.choose_action(s, episode)\n",
    "            s2, reward, done, _ = env.step(a)        \n",
    "            agent.learn(s, a, reward, s2)\n",
    "            total_reward += reward\n",
    "            s = s2\n",
    "            if done:\n",
    "                agent.end_state(s2, reward)               \n",
    "                break\n",
    "        rewards.append(total_reward)\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 676.  324.    0.    0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.    0.    0.    0.]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake8x8-v0')        \n",
    "states = np.zeros(64)\n",
    "for i in range(1000):\n",
    "    s = env.reset()\n",
    "    action = 3\n",
    "    s2, reward, done, _ = env.step(action)\n",
    "    states[s2] += 1\n",
    "#     print('%d: state=%d, action=%d, end_state=%d' % (i, s, action, s2))\n",
    "print(states.reshape((8,8)))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCB Table Agent\n",
    "\n",
    "The UCB tries to estimate the Q function value and variance. \n",
    "Then action is selected based on both value and variance.\n",
    "\n",
    "The formula to select action:\n",
    "\n",
    "$$ A_t = argmax_{a} \\left[ Q_t(a) + c \\sqrt{ \\frac{log(t)}{N_t(a)} } \\right] $$\n",
    "\n",
    "Where:\n",
    "  * $\\sqrt{ \\frac{log(t)}{N_t(a)} }$ - Is variance (uncertainty) of action value\n",
    "  * $N_t(a)$ - How many times this action was selected\n",
    "  * c - controls the degree of exploration\n",
    "  \n",
    "The formula to learn:\n",
    "\n",
    "$$ Q'_{a} = \\frac{Q_{a}*(n-1) + r}{n} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCBAgent:\n",
    "    \n",
    "    def __init__(self, num_states, num_actions, alpha, c=2.0):\n",
    "        self.qtable = np.zeros([num_states, num_actions])\n",
    "        self.num_actions = num_actions\n",
    "        self.c = c\n",
    "        self.gamma = 0.99\n",
    "        self.alpha = alpha\n",
    "        self.counters = np.zeros([num_states, num_actions])\n",
    "        \n",
    "    def choose_action(self, state, _):\n",
    "        if np.min(self.counters[state,:]) == 0:\n",
    "            return np.argmin(self.counters[state,:])\n",
    "        else:\n",
    "            t = np.sum(self.counters[state,:])\n",
    "            actions = self.c * np.sqrt(np.log(t)/self.counters[state,:])\n",
    "            return np.argmax(self.qtable[state,:] + actions)\n",
    "        \n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        \"\"\" Update state using SARSA(0)\n",
    "            Also add -1 to the reward to let agent learn shortest path\n",
    "        \"\"\"\n",
    "        next_action = self.choose_action(next_state, 0)\n",
    "        q = reward + self.gamma * self.qtable[next_state,next_action]\n",
    "        self.qtable[state, action] += self.alpha * (q-self.qtable[state, action])\n",
    "        self.counters[state, action] += 1    \n",
    "        \n",
    "    def end_state(self, state, reward):\n",
    "        \"\"\" Last state reached. \n",
    "        \"\"\"\n",
    "        if reward > 0:\n",
    "            self.qtable[state, :] = 1\n",
    "        else:\n",
    "            self.qtable[state, :] = -1\n",
    "        \n",
    "    def values(self):\n",
    "        return self.qtable.max(axis=1)        \n",
    "    \n",
    "    def policy(self):\n",
    "        return self.qtable.argmax(axis=1)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train agent\n",
    "\n",
    "Messing with the rewards:\n",
    "\n",
    "  * If we don't change reward then the probability of randomly finding solution is very low. \n",
    "    And the QTable learning will be very slow. And since there are 256 states, the propagation of\n",
    "    value is very slow\n",
    "  * By modifying reward we can try to at least try to not fall into the hole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found solution 2 times\n",
      "[[-0.579 -0.596 -0.582 -0.554 -0.492 -0.344 -0.223 -0.095]\n",
      " [-0.603 -0.613 -0.627 -0.622 -0.558 -0.434 -0.229 -0.138]\n",
      " [-0.623 -0.663 -0.69  -1.    -0.597 -0.473 -0.285 -0.136]\n",
      " [-0.682 -0.697 -0.741 -0.762 -0.442 -1.    -0.217 -0.087]\n",
      " [-0.679 -0.784 -0.861 -1.    -0.059 -0.039 -0.114 -0.069]\n",
      " [-0.555 -1.    -1.    -0.099 -0.038 -0.004 -1.    -0.001]\n",
      " [-0.184 -1.     0.     0.    -1.    -0.043 -1.     0.199]\n",
      " [-0.046 -0.013  0.    -1.     0.     0.     0.     1.   ]]\n",
      "Policy:\n",
      "[[3 3 3 3 3 3 2 2]\n",
      " [3 0 3 3 2 1 2 2]\n",
      " [3 0 0 0 2 3 2 2]\n",
      " [3 0 3 1 0 0 2 2]\n",
      " [0 3 2 0 2 1 3 2]\n",
      " [0 0 0 0 2 0 0 2]\n",
      " [0 0 0 0 0 3 0 2]\n",
      " [1 1 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Learning parameters\n",
    "learning_rate = 0.1\n",
    "solved_score = 0.99\n",
    "# Environment\n",
    "env = gym.make('FrozenLake8x8-v0')\n",
    "# Agent\n",
    "agent = UCBAgent(env.observation_space.n, env.action_space.n, learning_rate)\n",
    "found = 0\n",
    "for episode in range(int(1e6)):\n",
    "    s = env.reset()\n",
    "    while True:\n",
    "        a = agent.choose_action(s, episode)\n",
    "        s2, reward, done, _ = env.step(a)   \n",
    "        agent.learn(s, a, reward, s2)\n",
    "        s = s2\n",
    "        if done:\n",
    "            agent.end_state(s, reward)\n",
    "            break\n",
    "    if reward > 0:\n",
    "        found += 1\n",
    "        if found > 1:\n",
    "            break\n",
    "\n",
    "print('Found solution %d times' % found)        \n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(agent.values().reshape(8,8))\n",
    "print(\"Policy:\")\n",
    "print(agent.policy().reshape(8,8))\n",
    "env.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning parameters\n",
    "learning_rate = 0.01\n",
    "num_episodes = int(1e7)\n",
    "solved_score = 0.99\n",
    "# Environment\n",
    "env = gym.make('FrozenLake8x8-v0')\n",
    "# Agent\n",
    "agent = UCBAgent(env.observation_space.n, env.action_space.n, learning_rate)\n",
    "# run simulation\n",
    "rewards = run_env(agent, env, num_episodes)\n",
    "env.close()\n",
    "# Show summary\n",
    "averaged_rewards = moving_average(rewards)    \n",
    "idx = find_index(averaged_rewards, solved_score)\n",
    "print('Max score: %f' % np.max(averaged_rewards))\n",
    "if idx >= 0:\n",
    "    print('Solved after {} episodes'.format(idx+1))\n",
    "else:\n",
    "    print('Not solved')\n",
    "\n",
    "plt.plot(averaged_rewards)\n",
    "plt.show()\n",
    "print(\"Max Q value:\")\n",
    "print(agent.values().reshape(8,8))\n",
    "print(\"Policy:\")\n",
    "print(agent.policy().reshape(8,8))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
